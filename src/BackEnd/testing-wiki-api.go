package main

import (
	"encoding/json"
	"fmt"
	"io"
	"log"
	"net/http"
	"net/url"
	"strings"
	"time"

	"github.com/PuerkitoBio/goquery"
)

type ResponseAPI struct {
	Path    [][]string    `json:"path"`
	Status  bool          `json:"status"`
	Message string        `json:"message"`
	Time    time.Duration `json:"time"`
}

type Page struct {
	Title  string `json:"title"`
	Pageid int    `json:"pageid"`
	Url    string `json:"url"`
	Status bool   `json:"status"`
}

type AutoGenerated struct {
	Batchcomplete bool `json:"batchcomplete"`
	Continue      struct {
		Sroffset int    `json:"sroffset"`
		Continue string `json:"continue"`
	} `json:"continue"`
	Query struct {
		Searchinfo struct {
			Totalhits         int    `json:"totalhits"`
			Suggestion        string `json:"suggestion"`
			Suggestionsnippet string `json:"suggestionsnippet"`
		} `json:"searchinfo"`
		Search []struct {
			Ns        int       `json:"ns"`
			Title     string    `json:"title"`
			Pageid    int       `json:"pageid"`
			Size      int       `json:"size"`
			Wordcount int       `json:"wordcount"`
			Snippet   string    `json:"snippet"`
			Timestamp time.Time `json:"timestamp"`
		} `json:"search"`
	} `json:"query"`
}

func PrettyPrint(i interface{}) string {
	s, _ := json.MarshalIndent(i, "", "\t")
	return string(s)
}

func formatParams(params map[string]string) string {
	queryString := "?"
	for key, value := range params {
		queryString += key + "=" + url.QueryEscape(value) + "&"
	}
	return queryString[:len(queryString)-1]
}

func isIn(s string, arr []string) bool {
	for _, elmt := range arr {
		if elmt == s {
			return true
		}
	}
	return false
}

/*
* Fungsi untuk searching dan get recommendation
 */
func sendApi(search string) Page {

	apiUrl := "https://en.wikipedia.org/w/api.php"
	webUrl := "https://en.wikipedia.org/wiki/"

	params := map[string]string{
		"action":        "query",
		"format":        "json",
		"formatversion": "2",
		"list":          "search",
		"srsearch":      search,
	}

	resp, err := http.Get(apiUrl + formatParams(params))
	if err != nil {
		fmt.Println(err)
	}
	defer resp.Body.Close()
	body, err := io.ReadAll(resp.Body)

	var result AutoGenerated
	if err := json.Unmarshal(body, &result); err != nil {
		fmt.Println("Can not unmarshal JSON")
	}

	var ansPage Page

	if len(result.Query.Search) == 0 {
		ansPage.Status = false
		return ansPage
	}

	ansPage.Pageid = result.Query.Search[0].Pageid
	ansPage.Title = result.Query.Search[0].Title
	ansPage.Url = webUrl + strings.ReplaceAll(ansPage.Title, " ", "_")
	ansPage.Status = true

	return ansPage
}

// func toFile(res []byte) {
// 	os.WriteFile("result", res, 0644)
// }

// func toFileS(res []string) {
// 	f, err := os.Create("result")
// 	if err != nil {
// 		fmt.Println(err)
// 	}
// 	w := bufio.NewWriter(f)

// 	temp_string := ""

// 	for _, element := range res {
// 		temp_string = temp_string + "\n" + element
// 	}

// 	n4, err := w.WriteString(temp_string)
// 	fmt.Println(n4)
// 	w.Flush()
// }

func timeTrack(start time.Time, name string, executedTime *time.Duration) {
	elapsed := time.Since(start)
	*executedTime = elapsed
	log.Printf("%s took %s", name, elapsed)
}

func scrapWeb(url string) []string {
	resp, err := http.Get(url)
	if err != nil {
		fmt.Println(err)
	}
	defer resp.Body.Close()

	doc, err := goquery.NewDocumentFromReader(resp.Body)
	if err != nil {
		log.Fatal("Failed to parse the HTML document", err)
	}

	namespace_list := []string{"User:", "File:", "MediaWiki:", "Template:", "Help:", "Category:", "Special:", "Talk:", "Template_talk:", "Wikipedia:"}

	urlHTML := doc.Find("#content").Find("a").FilterFunction(func(i int, s *goquery.Selection) bool {
		link, _ := s.Attr("href")

		a := strings.HasPrefix(link, "/wiki/")

		for _, elmt := range namespace_list {
			a = a && !strings.Contains(link, elmt)
		}
		return a
	})

	final_ans := urlHTML.Map(func(i int, s *goquery.Selection) string {
		link, _ := s.Attr("href")

		return "https://en.wikipedia.org" + link
	})

	return final_ans
}

func DLS(start string, end string, maxdepth int, visited_dls map[string]bool, saved_path []string, ans *[][]string) bool {
	if start == end {
		*ans = append(*ans, saved_path)
		for _, elmt := range saved_path {
			fmt.Println(elmt)
		}
		return true
	}

	if maxdepth <= 0 {
		return false
	}

	url_scrap := scrapWeb(start)

	url_list := []string{}

	for _, elmt := range url_scrap {
		_, err := visited_dls[elmt]

		if !err {
			visited_dls[elmt] = true
			url_list = append(url_list, elmt)
		}
	}

	for _, elmt := range url_list {
		saved_path2 := append(saved_path, elmt)
		if DLS(elmt, end, maxdepth-1, visited_dls, saved_path2, ans) {
			return true
		}
	}
	return false
}

func IDS(start string, end string) *ResponseAPI {
	var resp ResponseAPI
	defer timeTrack(time.Now(), "IDS", &resp.Time)

	isFound := false
	saved_path := []string{}

	var i int = 0
	for !isFound {
		var visited_dls = map[string]bool{}
		if DLS(start, end, i, visited_dls, saved_path, &resp.Path) {
			isFound = true
			break
		}
		i++
	}

	resp.Status = isFound
	resp.Path = append(resp.Path, saved_path)
	if isFound {
		resp.Message = "Path Found"
		for _, elmt := range saved_path {
			fmt.Println(elmt)
		}
	} else {
		resp.Message = "Path not found"
	}

	return &resp
}

func BFS(url string, end string) *ResponseAPI {
	var resp ResponseAPI

	defer timeTrack(time.Now(), "scrapWeb", &resp.Time)

	url_queue := []string{}

	current_url := url

	for current_url != end {

		fmt.Println(current_url)

		link_res := scrapWeb(current_url)

		for _, elmt := range link_res {
			// check if it not exist
			if !isIn(elmt, url_queue) {
				url_queue = append(url_queue, elmt)
			}
		}

		current_url = url_queue[0]
		url_queue = url_queue[1:]

	}

	if current_url == end {
		resp.Status = true
		resp.Message = "Path Found"
	} else {
		resp.Status = false
		resp.Message = "No Possible Path Found"
	}

	fmt.Println("=======[ END ]=======")
	fmt.Println(current_url)

	return &resp
}

// func main() {
// 	page1 := sendApi("jokowi")
// 	page2 := sendApi("central java")

// 	// get initial value
// 	fmt.Println(PrettyPrint(page1))
// 	fmt.Println(PrettyPrint(page2))

// 	// start scraping
// 	max_depth := 3
// 	// BFS(page1.Url, page2.Url)
// 	x := IDS(page1.Url, page2.Url, max_depth)
// 	fmt.Println(x)
// }
